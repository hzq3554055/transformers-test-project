{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Transformers Testing Framework - Quick Start Guide\n",
        "\n",
        "Welcome to the **Transformers Testing Framework**! This comprehensive framework provides:\n",
        "\n",
        "- üåê **Online Mode**: Real Hugging Face models with proxy support\n",
        "- üîÑ **Offline Mode**: Mock models for development without internet\n",
        "- üñ•Ô∏è **GPU Support**: Automatic CUDA detection and acceleration\n",
        "- üìä **Performance Benchmarking**: CPU vs GPU comparisons\n",
        "- üß™ **Comprehensive Testing**: Full test suite and utilities\n",
        "- üìù **Data Processing**: Sample data creation and tokenization\n",
        "- üñ•Ô∏è **CLI Interface**: Command-line tools for automation\n",
        "- üìì **Jupyter Integration**: Interactive notebooks for development\n",
        "\n",
        "## üéØ What You'll Learn\n",
        "\n",
        "1. **Setup and Configuration** - Environment and proxy setup\n",
        "2. **GPU Detection** - Automatic CUDA detection and optimization\n",
        "3. **Offline Testing** - Mock models for development\n",
        "4. **Online Testing** - Real Hugging Face models with proxy\n",
        "5. **Performance Comparison** - CPU vs GPU benchmarking\n",
        "6. **Batch Processing** - Efficient batch inference\n",
        "7. **Memory Monitoring** - GPU memory usage tracking\n",
        "8. **Summary and Next Steps** - Framework overview and usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "First, let's set up the environment and check our configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the framework\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "\n",
        "from transformers_test import (\n",
        "    ModelTester, OfflineModelTester, DataProcessor,\n",
        "    setup_logging, get_device, set_seed,\n",
        "    get_gpu_info, get_optimal_device, get_memory_usage, print_gpu_status,\n",
        "    setup_huggingface_proxy, get_proxy_info\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logger = setup_logging(level='INFO')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "print(\"‚úÖ Framework imported successfully!\")\n",
        "print(f\"üéØ Optimal device: {get_optimal_device()}\")\n",
        "print(f\"üîß Current device: {get_device()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. GPU Detection and Status\n",
        "\n",
        "Let's check your GPU status and capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print GPU status\n",
        "print_gpu_status()\n",
        "\n",
        "# Get detailed GPU information\n",
        "gpu_info = get_gpu_info()\n",
        "print(f\"\\nüìä GPU Information:\")\n",
        "print(f\"  Available: {gpu_info['available']}\")\n",
        "print(f\"  Count: {gpu_info['count']}\")\n",
        "print(f\"  Current device: {gpu_info['current_device']}\")\n",
        "\n",
        "if gpu_info['available']:\n",
        "    for device in gpu_info['devices']:\n",
        "        print(f\"\\nüñ•Ô∏è  GPU {device['id']}: {device['name']}\")\n",
        "        print(f\"    Total memory: {device['total_memory_gb']:.1f} GB\")\n",
        "        print(f\"    Allocated: {device['memory_allocated_gb']:.1f} GB\")\n",
        "        print(f\"    Reserved: {device['memory_reserved_gb']:.1f} GB\")\n",
        "        print(f\"    Free: {device['memory_free_gb']:.1f} GB\")\n",
        "        print(f\"    Compute capability: {device['major']}.{device['minor']}\")\n",
        "        print(f\"    Multiprocessors: {device['multi_processor_count']}\")\n",
        "\n",
        "# Get memory usage\n",
        "memory_info = get_memory_usage()\n",
        "print(f\"\\nüíæ Memory Usage:\")\n",
        "print(f\"  CPU: {memory_info['cpu_used_gb']:.1f}GB / {memory_info['cpu_total_gb']:.1f}GB ({memory_info['cpu_percent']:.1f}%)\")\n",
        "\n",
        "if 'gpu_0_total_gb' in memory_info:\n",
        "    print(f\"  GPU: {memory_info['gpu_0_allocated_gb']:.1f}GB / {memory_info['gpu_0_total_gb']:.1f}GB\")\n",
        "    print(f\"  GPU Free: {memory_info['gpu_0_free_gb']:.1f}GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Offline Testing (No Internet Required)\n",
        "\n",
        "Test offline functionality with mock models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test offline functionality (works without internet)\n",
        "print(\"üîÑ Testing offline functionality...\")\n",
        "\n",
        "# Initialize offline tester\n",
        "offline_tester = OfflineModelTester()\n",
        "\n",
        "# Test inference\n",
        "text = \"This is an offline test sentence.\"\n",
        "result = offline_tester.test_inference(text)\n",
        "\n",
        "print(f\"‚úÖ Offline inference test passed!\")\n",
        "print(f\"Input: {result['input_text']}\")\n",
        "print(f\"Model: {result['model_name']}\")\n",
        "print(f\"Device: {result['device']}\")\n",
        "print(f\"Predictions: {result['predictions']}\")\n",
        "print(f\"Predicted class: {result['predicted_class']}\")\n",
        "\n",
        "# Test offline performance\n",
        "print(\"\\n‚ö° Testing offline performance...\")\n",
        "benchmark = offline_tester.benchmark_performance(\n",
        "    \"Offline performance test\",\n",
        "    num_runs=3,\n",
        "    warmup_runs=1\n",
        ")\n",
        "print(f\"Mean time: {benchmark['mean_time']:.4f}s\")\n",
        "print(f\"Min time: {benchmark['min_time']:.4f}s\")\n",
        "print(f\"Max time: {benchmark['max_time']:.4f}s\")\n",
        "\n",
        "# Get model info\n",
        "info = offline_tester.get_model_info()\n",
        "print(f\"\\nüìä Model Information:\")\n",
        "print(f\"Model: {info['model_name']}\")\n",
        "print(f\"Parameters: {info['num_parameters']:,}\")\n",
        "print(f\"Size: {info['model_size_mb']:.2f} MB\")\n",
        "print(f\"Architecture: {info['architecture']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Online Testing (With Internet Connection)\n",
        "\n",
        "Test online functionality with real Hugging Face models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test online functionality (requires internet connection)\n",
        "try:\n",
        "    print(\"üåê Testing online Hugging Face connection...\")\n",
        "    \n",
        "    # Initialize online model tester (will use GPU if available)\n",
        "    online_tester = ModelTester(\n",
        "        model_name='distilbert-base-uncased',\n",
        "        task_type='classification'\n",
        "    )\n",
        "    \n",
        "    # Test inference\n",
        "    text = \"This is an online test sentence.\"\n",
        "    result = online_tester.test_inference(text)\n",
        "    \n",
        "    print(f\"‚úÖ Online inference test passed!\")\n",
        "    print(f\"Input: {result['input_text']}\")\n",
        "    print(f\"Model: {result['model_name']}\")\n",
        "    print(f\"Device: {result['device']}\")\n",
        "    print(f\"Predictions: {result['predictions']}\")\n",
        "    print(f\"Predicted class: {result['predicted_class']}\")\n",
        "    \n",
        "    # Test online performance\n",
        "    print(\"\\n‚ö° Testing online performance...\")\n",
        "    benchmark = online_tester.benchmark_performance(\n",
        "        \"Online performance test\",\n",
        "        num_runs=3,\n",
        "        warmup_runs=1\n",
        "    )\n",
        "    print(f\"Mean time: {benchmark['mean_time']:.4f}s\")\n",
        "    print(f\"Min time: {benchmark['min_time']:.4f}s\")\n",
        "    print(f\"Max time: {benchmark['max_time']:.4f}s\")\n",
        "    \n",
        "    # Get model info\n",
        "    info = online_tester.get_model_info()\n",
        "    print(f\"\\nüìä Model Information:\")\n",
        "    print(f\"Model: {info['model_name']}\")\n",
        "    print(f\"Parameters: {info['num_parameters']:,}\")\n",
        "    print(f\"Size: {info['model_size_mb']:.2f} MB\")\n",
        "    print(f\"Architecture: {info['architecture']}\")\n",
        "    \n",
        "    online_available = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Online test failed: {e}\")\n",
        "    print(\"üîÑ Using offline mode instead...\")\n",
        "    online_available = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance Comparison (CPU vs GPU)\n",
        "\n",
        "Compare performance between CPU and GPU:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare CPU vs GPU performance\n",
        "if online_available:\n",
        "    print(\"üîÑ Comparing CPU vs GPU performance...\")\n",
        "    \n",
        "    # CPU test\n",
        "    print(\"  Testing CPU performance...\")\n",
        "    cpu_tester = ModelTester(\n",
        "        model_name='distilbert-base-uncased',\n",
        "        task_type='classification',\n",
        "        device='cpu'\n",
        "    )\n",
        "    cpu_benchmark = cpu_tester.benchmark_performance(\n",
        "        \"CPU vs GPU performance test\",\n",
        "        num_runs=3,\n",
        "        warmup_runs=1\n",
        "    )\n",
        "    \n",
        "    # GPU test\n",
        "    print(\"  Testing GPU performance...\")\n",
        "    gpu_tester = ModelTester(\n",
        "        model_name='distilbert-base-uncased',\n",
        "        task_type='classification',\n",
        "        device='cuda'\n",
        "    )\n",
        "    gpu_benchmark = gpu_tester.benchmark_performance(\n",
        "        \"CPU vs GPU performance test\",\n",
        "        num_runs=3,\n",
        "        warmup_runs=1\n",
        "    )\n",
        "    \n",
        "    # Compare results\n",
        "    speedup = cpu_benchmark['mean_time'] / gpu_benchmark['mean_time']\n",
        "    \n",
        "    print(f\"\\nüìä Performance Comparison:\")\n",
        "    print(f\"  CPU Mean time: {cpu_benchmark['mean_time']:.4f}s\")\n",
        "    print(f\"  GPU Mean time: {gpu_benchmark['mean_time']:.4f}s\")\n",
        "    print(f\"  Speedup: {speedup:.2f}x faster on GPU\")\n",
        "    \n",
        "    if speedup > 1.5:\n",
        "        print(\"  ‚úÖ GPU provides significant speedup!\")\n",
        "    elif speedup > 1.1:\n",
        "        print(\"  ‚úÖ GPU provides moderate speedup\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è  GPU speedup is minimal (might be due to small model size)\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Online testing not available, skipping performance comparison\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Batch Processing on GPU\n",
        "\n",
        "Test efficient batch processing on GPU:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch processing on GPU\n",
        "if online_available:\n",
        "    print(\"üîÑ Testing batch processing on GPU...\")\n",
        "    \n",
        "    # Initialize GPU tester\n",
        "    gpu_tester = ModelTester(\n",
        "        model_name='distilbert-base-uncased',\n",
        "        task_type='classification',\n",
        "        device='cuda'\n",
        "    )\n",
        "    \n",
        "    # Test batch inference\n",
        "    texts = [\n",
        "        \"This is the first test sentence.\",\n",
        "        \"This is the second test sentence.\",\n",
        "        \"This is the third test sentence.\",\n",
        "        \"This is the fourth test sentence.\",\n",
        "        \"This is the fifth test sentence.\"\n",
        "    ]\n",
        "    \n",
        "    batch_results = gpu_tester.test_batch_inference(texts, batch_size=2)\n",
        "    print(f\"‚úÖ Batch processing successful!\")\n",
        "    print(f\"   Processed {len(batch_results)} batches\")\n",
        "    print(f\"   Total texts: {len(texts)}\")\n",
        "    \n",
        "    # Show results\n",
        "    for i, result in enumerate(batch_results):\n",
        "        print(f\"   Batch {i+1}: {len(result['input_text'])} texts processed\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Online testing not available, skipping batch processing test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Memory Monitoring\n",
        "\n",
        "Monitor GPU memory usage during operations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor memory usage\n",
        "print(\"üíæ Memory Usage Monitoring:\")\n",
        "\n",
        "# Get initial memory usage\n",
        "initial_memory = get_memory_usage()\n",
        "print(f\"Initial CPU: {initial_memory['cpu_used_gb']:.1f}GB / {initial_memory['cpu_total_gb']:.1f}GB ({initial_memory['cpu_percent']:.1f}%)\")\n",
        "\n",
        "if 'gpu_0_total_gb' in initial_memory:\n",
        "    print(f\"Initial GPU: {initial_memory['gpu_0_allocated_gb']:.1f}GB / {initial_memory['gpu_0_total_gb']:.1f}GB\")\n",
        "    print(f\"Initial GPU Free: {initial_memory['gpu_0_free_gb']:.1f}GB\")\n",
        "\n",
        "# Test memory usage with model loading\n",
        "if online_available:\n",
        "    print(\"\\nüîÑ Testing memory usage with model loading...\")\n",
        "    \n",
        "    # Load model and check memory\n",
        "    test_tester = ModelTester(\n",
        "        model_name='distilbert-base-uncased',\n",
        "        task_type='classification',\n",
        "        device='cuda'\n",
        "    )\n",
        "    \n",
        "    # Get memory after model loading\n",
        "    after_loading_memory = get_memory_usage()\n",
        "    print(f\"After loading CPU: {after_loading_memory['cpu_used_gb']:.1f}GB / {after_loading_memory['cpu_total_gb']:.1f}GB ({after_loading_memory['cpu_percent']:.1f}%)\")\n",
        "    \n",
        "    if 'gpu_0_total_gb' in after_loading_memory:\n",
        "        print(f\"After loading GPU: {after_loading_memory['gpu_0_allocated_gb']:.1f}GB / {after_loading_memory['gpu_0_total_gb']:.1f}GB\")\n",
        "        print(f\"After loading GPU Free: {after_loading_memory['gpu_0_free_gb']:.1f}GB\")\n",
        "        \n",
        "        # Calculate memory increase\n",
        "        gpu_increase = after_loading_memory['gpu_0_allocated_gb'] - initial_memory['gpu_0_allocated_gb']\n",
        "        print(f\"GPU memory increase: {gpu_increase:.1f}GB\")\n",
        "        \n",
        "        # Test inference and check memory\n",
        "        result = test_tester.test_inference(\"Memory test sentence\")\n",
        "        after_inference_memory = get_memory_usage()\n",
        "        \n",
        "        if 'gpu_0_total_gb' in after_inference_memory:\n",
        "            print(f\"After inference GPU: {after_inference_memory['gpu_0_allocated_gb']:.1f}GB / {after_inference_memory['gpu_0_total_gb']:.1f}GB\")\n",
        "            print(f\"After inference GPU Free: {after_inference_memory['gpu_0_free_gb']:.1f}GB\")\n",
        "            \n",
        "            # Calculate total memory used\n",
        "            total_gpu_used = after_inference_memory['gpu_0_allocated_gb'] - initial_memory['gpu_0_allocated_gb']\n",
        "            print(f\"Total GPU memory used: {total_gpu_used:.1f}GB\")\n",
        "            \n",
        "            # Check if we're using reasonable amount of memory\n",
        "            if total_gpu_used < 1.0:\n",
        "                print(\"‚úÖ Memory usage is reasonable\")\n",
        "            elif total_gpu_used < 2.0:\n",
        "                print(\"‚ö†Ô∏è  Memory usage is moderate\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  Memory usage is high - consider using smaller models\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Online testing not available, skipping memory monitoring test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Next Steps\n",
        "\n",
        "Your transformers testing framework is now fully configured with GPU support!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(\"üéâ Transformers Testing Framework - Complete Setup!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n‚úÖ What's Working:\")\n",
        "print(\"  üåê Online Mode: Real Hugging Face models with proxy support\")\n",
        "print(\"  üîÑ Offline Mode: Mock models for development\")\n",
        "print(\"  üñ•Ô∏è  GPU Support: Automatic CUDA detection and acceleration\")\n",
        "print(\"  üìä Performance: CPU vs GPU benchmarking\")\n",
        "print(\"  üíæ Memory Monitoring: GPU memory usage tracking\")\n",
        "print(\"  üîÑ Batch Processing: Efficient batch inference\")\n",
        "print(\"  üß™ Testing: Comprehensive test suite\")\n",
        "\n",
        "print(\"\\nüöÄ Available Features:\")\n",
        "print(\"  üìì Jupyter Notebooks: Interactive development\")\n",
        "print(\"  üñ•Ô∏è  CLI Interface: Command-line tools\")\n",
        "print(\"  üìä Performance Benchmarking: CPU vs GPU comparisons\")\n",
        "print(\"  üíæ Memory Management: GPU memory monitoring\")\n",
        "print(\"  üîÑ Batch Processing: Efficient inference\")\n",
        "print(\"  üåê Proxy Support: Hugging Face Hub access\")\n",
        "print(\"  üîÑ Offline Fallback: Development without internet\")\n",
        "\n",
        "print(\"\\nüìã Next Steps:\")\n",
        "print(\"  1. üöÄ Start Jupyter: jupyter notebook\")\n",
        "print(\"  2. üìì Open notebooks/01_quick_start.ipynb\")\n",
        "print(\"  3. üß™ Run tests: python -m pytest tests/ -v\")\n",
        "print(\"  4. üñ•Ô∏è  Use CLI: python -m transformers_test.cli --help\")\n",
        "print(\"  5. üìä Benchmark: ./scripts/test_gpu.sh\")\n",
        "print(\"  6. üåê Upload to GitHub for sharing\")\n",
        "\n",
        "print(\"\\nüéØ Your RTX 3060 is now fully integrated!\")\n",
        "print(\"   GPU acceleration: ‚úÖ Enabled\")\n",
        "print(\"   Proxy support: ‚úÖ Configured\")\n",
        "print(\"   Offline mode: ‚úÖ Available\")\n",
        "print(\"   Performance: ‚úÖ Optimized\")\n",
        "\n",
        "print(\"\\nüöÄ Happy coding with your GPU-accelerated transformers framework!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Start Guide\n",
        "\n",
        "This notebook demonstrates the basic usage of the transformers testing framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the framework\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "\n",
        "from transformers_test import ModelTester, DataProcessor, setup_logging\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup logging\n",
        "logger = setup_logging(level='INFO')\n",
        "logger.info('Transformers testing framework initialized')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Model Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model tester\n",
        "tester = ModelTester(\n",
        "    model_name='bert-base-uncased',\n",
        "    task_type='classification'\n",
        ")\n",
        "\n",
        "# Test inference\n",
        "text = \"Hello world, this is a test sentence.\"\n",
        "result = tester.test_inference(text)\n",
        "\n",
        "print(f\"Input: {result['input_text']}\")\n",
        "print(f\"Model: {result['model_name']}\")\n",
        "print(f\"Device: {result['device']}\")\n",
        "print(f\"Input length: {result['input_length']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Offline Testing (No Internet Required)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test offline functionality (works without internet)\n",
        "from transformers_test import OfflineModelTester, setup_logging, set_seed\n",
        "\n",
        "# Setup\n",
        "logger = setup_logging(level='INFO')\n",
        "set_seed(42)\n",
        "\n",
        "# Initialize offline tester\n",
        "offline_tester = OfflineModelTester()\n",
        "\n",
        "# Test inference\n",
        "text = \"This is an offline test sentence.\"\n",
        "result = offline_tester.test_inference(text)\n",
        "\n",
        "print(f\"Input: {result['input_text']}\")\n",
        "print(f\"Model: {result['model_name']}\")\n",
        "print(f\"Device: {result['device']}\")\n",
        "print(f\"Predictions: {result['predictions']}\")\n",
        "print(f\"Predicted class: {result['predicted_class']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Performance Benchmarking (Offline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark offline model performance\n",
        "benchmark_results = offline_tester.benchmark_performance(\n",
        "    text=\"This is a benchmark test sentence.\",\n",
        "    num_runs=5,\n",
        "    warmup_runs=2\n",
        ")\n",
        "\n",
        "print(f\"Benchmark Results:\")\n",
        "print(f\"  Mean time: {benchmark_results['mean_time']:.4f}s\")\n",
        "print(f\"  Min time: {benchmark_results['min_time']:.4f}s\")\n",
        "print(f\"  Max time: {benchmark_results['max_time']:.4f}s\")\n",
        "print(f\"  Std time: {benchmark_results['std_time']:.4f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Information (Offline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get model information\n",
        "model_info = offline_tester.get_model_info()\n",
        "\n",
        "print(f\"Model Information:\")\n",
        "print(f\"  Model: {model_info['model_name']}\")\n",
        "print(f\"  Task: {model_info['task_type']}\")\n",
        "print(f\"  Device: {model_info['device']}\")\n",
        "print(f\"  Parameters: {model_info['num_parameters']:,}\")\n",
        "print(f\"  Model size: {model_info['model_size_mb']:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Online Testing (With Internet Connection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test online functionality (requires internet connection)\n",
        "try:\n",
        "    print(\"üåê Testing online Hugging Face connection...\")\n",
        "    \n",
        "    # Initialize online model tester\n",
        "    online_tester = ModelTester(\n",
        "        model_name='bert-base-uncased',\n",
        "        task_type='classification'\n",
        "    )\n",
        "    \n",
        "    # Test inference\n",
        "    text = \"This is an online test sentence.\"\n",
        "    result = online_tester.test_inference(text)\n",
        "    \n",
        "    print(f\"‚úÖ Online inference test passed!\")\n",
        "    print(f\"Input: {result['input_text']}\")\n",
        "    print(f\"Model: {result['model_name']}\")\n",
        "    print(f\"Device: {result['device']}\")\n",
        "    print(f\"Predictions: {result['predictions']}\")\n",
        "    print(f\"Predicted class: {result['predicted_class']}\")\n",
        "    \n",
        "    online_available = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Online test failed: {e}\")\n",
        "    print(\"üîÑ Using offline mode instead...\")\n",
        "    online_available = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare online vs offline performance\n",
        "if online_available:\n",
        "    print(\"üìä Comparing Online vs Offline Performance...\")\n",
        "    \n",
        "    # Online benchmark\n",
        "    online_benchmark = online_tester.benchmark_performance(\n",
        "        \"This is a performance comparison test.\",\n",
        "        num_runs=3,\n",
        "        warmup_runs=1\n",
        "    )\n",
        "    \n",
        "    # Offline benchmark\n",
        "    offline_benchmark = offline_tester.benchmark_performance(\n",
        "        \"This is a performance comparison test.\",\n",
        "        num_runs=3,\n",
        "        warmup_runs=1\n",
        "    )\n",
        "    \n",
        "    print(f\"Online Model Performance:\")\n",
        "    print(f\"  Mean time: {online_benchmark['mean_time']:.4f}s\")\n",
        "    print(f\"  Model: {online_tester.get_model_info()['model_name']}\")\n",
        "    print(f\"  Parameters: {online_tester.get_model_info()['num_parameters']:,}\")\n",
        "    \n",
        "    print(f\"\\nOffline Model Performance:\")\n",
        "    print(f\"  Mean time: {offline_benchmark['mean_time']:.4f}s\")\n",
        "    print(f\"  Model: {offline_tester.get_model_info()['model_name']}\")\n",
        "    print(f\"  Parameters: {offline_tester.get_model_info()['num_parameters']:,}\")\n",
        "    \n",
        "    speed_ratio = online_benchmark['mean_time'] / offline_benchmark['mean_time']\n",
        "    print(f\"\\nSpeed ratio (Online/Offline): {speed_ratio:.2f}x\")\n",
        "    \n",
        "else:\n",
        "    print(\"üìä Offline Performance Only:\")\n",
        "    offline_benchmark = offline_tester.benchmark_performance(\n",
        "        \"This is a performance test.\",\n",
        "        num_runs=3,\n",
        "        warmup_runs=1\n",
        "    )\n",
        "    print(f\"  Mean time: {offline_benchmark['mean_time']:.4f}s\")\n",
        "    print(f\"  Model: {offline_tester.get_model_info()['model_name']}\")\n",
        "    print(f\"  Parameters: {offline_tester.get_model_info()['num_parameters']:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(\"üéâ Transformers Testing Framework Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if online_available:\n",
        "    print(\"‚úÖ Online mode: Available (Hugging Face connection working)\")\n",
        "    print(\"‚úÖ Offline mode: Available (Mock models for development)\")\n",
        "    print(\"\\nüöÄ You can use both modes:\")\n",
        "    print(\"  - Online: Real Hugging Face models for production\")\n",
        "    print(\"  - Offline: Mock models for development and testing\")\n",
        "else:\n",
        "    print(\"‚úÖ Offline mode: Available (Mock models for development)\")\n",
        "    print(\"‚ö†Ô∏è  Online mode: Not available (No internet connection)\")\n",
        "    print(\"\\nüîÑ You can still develop and test using offline mode\")\n",
        "    print(\"   When internet is available, you can use real models\")\n",
        "\n",
        "print(\"\\nüìã Available Features:\")\n",
        "print(\"  ‚úÖ Model inference testing\")\n",
        "print(\"  ‚úÖ Performance benchmarking\")\n",
        "print(\"  ‚úÖ Batch processing\")\n",
        "print(\"  ‚úÖ Model information extraction\")\n",
        "print(\"  ‚úÖ Data processing utilities\")\n",
        "print(\"  ‚úÖ Training capabilities\")\n",
        "print(\"  ‚úÖ CLI interface\")\n",
        "print(\"  ‚úÖ Jupyter notebook integration\")\n",
        "\n",
        "print(\"\\nüöÄ Next Steps:\")\n",
        "print(\"  1. Explore the notebooks/ directory\")\n",
        "print(\"  2. Run tests: python -m pytest tests/ -v\")\n",
        "print(\"  3. Use CLI: python -m transformers_test.cli --help\")\n",
        "print(\"  4. Check documentation: docs/API.md\")\n",
        "print(\"  5. Upload to GitHub for sharing\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
